# #
# # import torch
# # import torch.nn as nn
# # import torch.nn.functional as F
# #
# # class GSDFAWrapper(nn.Module):
# #     def __init__(self, model, mask_old=None):
# #         super(GSDFAWrapper, self).__init__()
# #         self.model = model
# #         self.device = next(self.model.parameters()).device
# #         self.mask_old = mask_old  # å†å²mask
# #         self.mask = None  # å½“å‰mask
# #         self.hook_registered = False
# #
# #     def register_feature_hook(self):
# #         def feature_hook(module, input, output):
# #             if self.mask_old is not None:
# #                 if self.mask_old.shape[0] != output.shape[1]:
# #                     # print(
# #                     #     f"[è­¦å‘Š] Mask channels ({self.mask_old.shape[0]}) != Output channels ({output.shape[1]}), è‡ªåŠ¨é€‚é…")
# #                     if self.mask_old.shape[0] > output.shape[1]:
# #                         adjusted_mask = self.mask_old[:output.shape[1]]
# #                     else:
# #                         adjusted_mask = F.pad(self.mask_old, (0, output.shape[1] - self.mask_old.shape[0]), "constant",
# #                                               1.0)
# #                 else:
# #                     adjusted_mask = self.mask_old
# #                 mask = adjusted_mask.view(1, -1, 1, 1)
# #                 return output * mask
# #             else:
# #                 return output
# #         # è‡ªåŠ¨æŒ‚åˆ° model.feature_layerï¼Œå¦‚æœæ²¡æœ‰éœ€è¦ç”¨æˆ·åœ¨æ¨¡å‹é‡Œè¡¥å……
# #         if hasattr(self.model, 'feature_layer'):
# #             self.model.feature_layer.register_forward_hook(feature_hook)
# #         else:
# #             raise AttributeError("Model has no attribute 'feature_layer'. Please set 'feature_layer' manually.")
# #
# #     def extract_features(self, x):
# #         if not self.hook_registered:
# #             self.register_feature_hook()
# #             self.hook_registered = True
# #         return self.model.extract_features(x)
# #
# #     def forward(self, x):
# #         feat = self.extract_features(x)
# #         feat = feat.view(feat.size(0), -1)  # <-- ğŸ”¥é‡è¦ï¼Œå±•å¹³ï¼ï¼
# #         feat = F.relu(self.model.fc1(feat))
# #         logits = self.model.fc2(feat)
# #         return logits
# #
# #     def update(self, dataloader, epochs=5, k=5, lr=1e-4):
# #         self.model.train()
# #         optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)
# #         for epoch in range(epochs):
# #             for batch_idx, (x, _) in enumerate(dataloader):
# #                 x = x.to(self.device)
# #                 feat = self.extract_features(x)
# #                 logits = self.model.classify(feat)
# #                 entropy = -torch.sum(F.softmax(logits, dim=1) * F.log_softmax(logits, dim=1), dim=1).mean()
# #                 loss = entropy
# #                 optimizer.zero_grad()
# #                 loss.backward()
# #                 optimizer.step()
# #
# # def setup_gsdfa(model, mask_old=None):
# #     return GSDFAWrapper(model, mask_old)

# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
#
#
# class SDFAAdapt:
#     def __init__(self, model, classifier, mask_old=None, reg_lambda=0.001, max_iter=100, k=3):
#         self.model = model  # æºæ¨¡å‹ï¼ˆç‰¹å¾æå–å™¨ï¼‰
#         self.classifier = classifier  # åˆ†ç±»å™¨ï¼ˆfc1 + fc2ï¼‰
#         self.mask_old = mask_old  # åˆå§‹æ©ç ï¼ˆå¯é€‰ï¼‰
#         self.reg_lambda = reg_lambda  # æ­£åˆ™åŒ–æƒé‡
#         self.max_iter = max_iter  # æœ€å¤§è¿­ä»£æ¬¡æ•°
#         self.k = k  # é‚»åŸŸæ•°é‡
#         # å†»ç»“ç‰¹å¾æå–å™¨çš„å‚æ•°
#         for param in self.model.parameters():
#             param.requires_grad = False
#         # ç¡®ä¿åˆ†ç±»å™¨å‚æ•°å¯è®­ç»ƒ
#         for param in self.classifier.parameters():
#             param.requires_grad = True  # æ·»åŠ æ­¤è¡Œ
#
#         # å†»ç»“ç‰¹å¾æå–å™¨çš„å‚æ•°
#         for param in self.model.parameters():
#             param.requires_grad = False
#
#         # ä»…ä¼˜åŒ–åˆ†ç±»å™¨å‚æ•°
#         self.optimizer = optim.SGD(self.classifier.parameters(), lr=1e-6)
#         self.device = next(self.model.parameters()).device
#
#         # åŠ¨æ€è®¡ç®— fc1 è¾“å…¥å°ºå¯¸
#         self._to_linear = None
#         self._calculate_flattened_size()
#
#     def _calculate_flattened_size(self):
#         with torch.no_grad():
#             x = torch.randn(1, 1, 2000, 30).to(self.device)
#             features = self.model.extract_features(x)
#             self._to_linear = features.numel()
#
#     def compute_maximization_loss(self, features):
#         feat_mean = features.mean(dim=0)
#         feat_diff = features - feat_mean
#         feat_loss = feat_diff.pow(2).sum(dim=1).mean()
#         return feat_loss
#
#     def generate_dynamic_mask(self, features):
#         # åŠ¨æ€ç”Ÿæˆæ©ç ï¼ˆç¤ºä¾‹ï¼šåŸºäºç‰¹å¾å‡å€¼ç”Ÿæˆï¼‰
#         embedding = features.mean(dim=[2, 3])  # å…¨å±€å¹³å‡æ± åŒ–
#         mask = torch.sigmoid(100 * embedding)  # ç”Ÿæˆæ¥è¿‘äºŒå€¼çš„æ©ç 
#         return mask
#
#     def apply_gradient_mask(self):
#         if self.mask is None:
#             return
#         # åº”ç”¨åŠ¨æ€ç”Ÿæˆçš„æ©ç åˆ°åˆ†ç±»å™¨æ¢¯åº¦
#         for name, param in self.classifier.named_parameters():
#             if 'weight' in name and param.grad is not None:
#                 param.grad *= (1.0 - self.mask.view(-1, 1).to(self.device))
#             elif 'bias' in name and param.grad is not None:
#                 param.grad *= (1.0 - self.mask.to(self.device))
#
#     def train(self, dset_loader_target):
#         self.model.eval()  # ç‰¹å¾æå–å™¨å§‹ç»ˆåœ¨è¯„ä¼°æ¨¡å¼
#         self.classifier.train()  # åˆ†ç±»å™¨å¯è®­ç»ƒ
#
#         # åˆå§‹åŒ–ç‰¹å¾åº“å’Œå¾—åˆ†åº“
#         num_sample = len(dset_loader_target.dataset)
#         fea_bank = torch.randn(num_sample, self._to_linear).to(self.device)
#         score_bank = torch.randn(num_sample, self.classifier[-1].out_features).to(self.device)
#
#         # é¢„å¡«å……ç‰¹å¾åº“å’Œå¾—åˆ†åº“
#         with torch.no_grad():
#             for batch_idx, (x, _) in enumerate(dset_loader_target):
#                 x = x.to(self.device)
#                 features = self.model.extract_features(x)
#                 features_flat = features.view(features.size(0), -1)
#                 logits = self.classifier(features_flat)
#                 probs = F.softmax(logits, dim=1)
#
#                 start = batch_idx * dset_loader_target.batch_size
#                 end = start + x.size(0)
#                 fea_bank[start:end] = features_flat
#                 score_bank[start:end] = probs
#
#         for iter_num in range(self.max_iter):
#             total_loss_value = 0
#             for batch_idx, (x, _) in enumerate(dset_loader_target):
#                 x = x.to(self.device)
#
#                 # å‰å‘ä¼ æ’­
#                 with torch.no_grad():
#                     features = self.model.extract_features(x)
#                 features_flat = features.view(features.size(0), -1)
#                 logits = self.classifier(features_flat)
#                 probs = F.softmax(logits, dim=1)
#
#                 # è®¡ç®—ä¿¡æ¯æœ€å¤§åŒ–æŸå¤±
#                 im_loss = self.compute_maximization_loss(features)
#
#                 # è®¡ç®—é‚»åŸŸä¸€è‡´æ€§æŸå¤±
#                 start = batch_idx * dset_loader_target.batch_size
#                 end = start + x.size(0)
#                 fea_bank[start:end].fill_(-1)  # æ’é™¤å½“å‰æ‰¹æ¬¡
#                 distance = features_flat @ fea_bank.T
#                 _, idx_near = torch.topk(distance, self.k, dim=1)
#                 score_near = score_bank[idx_near].permute(0, 2, 1)
#                 output_re = probs.unsqueeze(1)
#                 loss_const = -torch.log(torch.bmm(output_re, score_near)).sum(-1).mean()
#
#                 # è®¡ç®—KLæ•£åº¦æŸå¤±ï¼ˆç±»åˆ«åˆ†å¸ƒå¯¹é½ï¼‰
#                 probs_mean = torch.mean(probs, dim=0)
#                 uniform_dist = torch.ones_like(probs_mean) / probs.size(1)
#                 kl_loss = F.kl_div(torch.log(probs_mean + 1e-6), uniform_dist, reduction='batchmean')
#
#                 # æ€»æŸå¤±
#                 total_loss = self.reg_lambda * im_loss + 0.1 * loss_const + 0.1 * kl_loss
#
#                 # åå‘ä¼ æ’­
#                 self.optimizer.zero_grad()
#                 total_loss.backward()
#
#                 # åº”ç”¨åŠ¨æ€æ¢¯åº¦æ©ç 
#                 self.mask = self.generate_dynamic_mask(features)
#                 self.apply_gradient_mask()
#
#                 # æ¢¯åº¦è£å‰ª
#                 torch.nn.utils.clip_grad_norm_(self.classifier.parameters(), max_norm=1.0)
#                 self.optimizer.step()
#
#                 total_loss_value += total_loss.item() * x.size(0)
#
#             avg_loss = total_loss_value / num_sample
#             print(f"Iteration {iter_num + 1}/{self.max_iter} - Loss: {avg_loss:.4f}")
#
#         return self.model, self.classifier

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim


class SDFAAdapt:
    def __init__(self, model, classifier, mask_old=None, reg_lambda=0.001, max_iter=100):
        self.model = model
        self.classifier = classifier
        self.mask_old = mask_old
        self.reg_lambda = reg_lambda
        self.max_iter = max_iter
        self.optimizer = optim.SGD(self.model.parameters(), lr=1e-4)
        self.device = next(self.model.parameters()).device
        self._to_linear = None
        self._calculate_flattened_size()

    def _calculate_flattened_size(self):
        with torch.no_grad():
            x = torch.randn(1, 1, 2000, 30).to(self.device)
            features = self.model.extract_features(x)
            self._to_linear = features.numel()

    def compute_kl_loss(self, probs, smoothed_labels):
        softmax_out = F.softmax(probs, dim=1)
        kl_loss = F.kl_div(torch.log(softmax_out + 1e-6), smoothed_labels, reduction='batchmean')
        return kl_loss

    def compute_maximization_loss(self, features):
        feat_mean = features.mean(dim=0)
        feat_diff = features - feat_mean
        feat_loss = feat_diff.pow(2).sum(dim=1).mean()
        return feat_loss

    def apply_gradient_mask(self):
        if self.mask_old is None:
            return
        for name, param in self.model.named_parameters():
            if 'fc1.weight' in name and param.grad is not None:
                param.grad *= (1.0 - self.mask_old.view(-1, 1).to(self.device))
            if 'fc1.bias' in name and param.grad is not None:
                param.grad *= (1.0 - self.mask_old.to(self.device))
            if 'fc2.weight' in name and param.grad is not None:
                param.grad *= (1.0 - self.mask_old.view(1, -1).to(self.device))

    def train(self, dset_loader_target):
        self.model.train()
        total_loss_value = 0
        total_samples = 0

        for iter_num in range(self.max_iter):
            for x_target, _ in dset_loader_target:  # å¿½ç•¥æ ‡ç­¾
                x_target = x_target.to(self.device)

                # å‰å‘ä¼ æ’­
                features = self.model.extract_features(x_target)
                features_flat = features.view(features.size(0), -1)
                logits = self.classifier(features_flat)
                probs = F.softmax(logits, dim=1)

                # ç”Ÿæˆä¼ªæ ‡ç­¾ï¼ˆåŸºäºé¢„æµ‹ç»“æœï¼‰
                _, pseudo_labels = torch.max(probs.detach(), dim=1)

                # åˆ›å»ºå¹³æ»‘æ ‡ç­¾ï¼ˆæ— ç›‘ç£ç‰ˆæœ¬ï¼‰
                smoothed_labels = torch.full_like(probs, fill_value=0.1 / probs.size(1))
                smoothed_labels.scatter_(1, pseudo_labels.unsqueeze(1), 0.9)

                # è®¡ç®—æŸå¤±
                kl_loss = self.compute_kl_loss(probs, smoothed_labels)
                im_loss = self.compute_maximization_loss(features)
                total_loss = kl_loss + self.reg_lambda * im_loss

                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                total_loss.backward()
                self.apply_gradient_mask()
                self.optimizer.step()

                total_loss_value += total_loss.item() * x_target.size(0)
                total_samples += x_target.size(0)

            avg_loss = total_loss_value / total_samples
            print(f"Iteration {iter_num + 1}/{self.max_iter} - Loss: {avg_loss:.4f}")

        return self.model, self.classifier